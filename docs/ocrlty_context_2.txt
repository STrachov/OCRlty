# OCRlty — Контекст и план (GPU: Arctic‑TILT на vLLM 0.8.3, CUDA 12.4, Torch 2.6.0)

## TL;DR
  - Используем **кастомный сервер `tilt_api.py`** (Uvicorn, порт **:8001**) вместо стандартного `vllm.entrypoints.openai.api_server` — это устраняет ошибки `TiltModel.forward(... encoder_chunk_ids ...)` и позволяет явно указать **задачу `tilt_generate`**.
  - **Бэкенд внимания:** используем **XFormers**. В рантайме выставляем:
    - `VLLM_ATTENTION_BACKEND=XFORMERS`
    - `VLLM_USE_FLASH_ATTENTION=0`
  - **Сборка:** vLLM **0.8.3** собран отдельно в wheel (CPython 3.10, Linux x86_64), выкладывается в **GitHub Releases**. В прод‑образе этот wheel подтягивается по URL + проверка **SHA256**.
  - **База образа:** CUDA **12.4** (runtime), Python **3.10**, **Torch 2.6.0+cu124**.
  - **Кастомная сборка XFormers.
  - **Prod на RunPod:** один контейнер, где поднимаем **`tilt_api.py` (:8001)**. (Второй API с OCR на CPU можно добавить позже, но TILT уже работает стабильно отдельно.)
  - **SHM:** в RunPod оставляем дефолтный **/dev/shm ≈ 24ГБ** (не занижаем, это важно для FA/xformers). 
  - **Отладка старта:** переменная `SLEEP_ON_START=1` переведёт контейнер в режим `tail -f /dev/null`, чтобы подключиться и проверить окружение.

## 1) Архитектура (минимально‑жизнеспособная для TILT)
  **Один под RunPod → один контейнер → один процесс:**
  - **`tilt_api.py` (Uvicorn, :8001)** — поднимает vLLM `LLM()` с `task=tilt_generate`, dtype=float16, TP=1; даёт OpenAI‑совместимый endpoint `/v1/chat/completions`.

  > Позже можно добавить второй процесс (FastAPI OCR на :8000) и сделать полноценный pipeline OCR → TILT. Текущая версия документа фокусируется на стабильном запуске **TILT**.


## 2) Версии/стек
  - **CUDA:** 12.4 (runtime)
  - **Python:** 3.10
  - **PyTorch:** 2.6.0+cu124 (`--extra-index-url https://download.pytorch.org/whl/cu124`)
  - **xformers:** 0.0.29.post2 (совместима с Torch 2.6.0 cu124)
  - **vLLM:** 0.8.3 (wheel, собранный из форка Snowflake‑Labs/arctic‑tilt v0.8.3)
  - **HF экосистема:** `huggingface_hub`, `tokenizers`, `sentencepiece` и т.д.

## 3) Колесо vLLM 0.8.3 (форк Arctic‑TILT) — сборка и выкладка
  - Сборка делалась на выделенной сервере (16 ядер, 64 ГБ RAM, быстрый NVMe). Итоговый wheel ≈ **400–420 МБ**. Процесс сборки подробно описан в файле vllm_wheel_creation.txt
  - Колесо кладём в **GitHub Releases** своего репозитория (тег `tilt-vllm-cu124-py310-torch26`).

## 4) GitHub Actions — заметки по сборке
  - Использовать **buildx** и **QEMU** не требуется (linux/amd64).
  - Чистить диск в CI перед установкой Torch (удаление `pip`‑кэшей, `rm -rf /usr/local/cuda-*/compat/*` не трогаем).
  - Сохранять артефактом **wheel** vLLM и выкладывать в **Releases** (имя файла без пробелов, полное имя колеса).
  - Здесь впервые задаются:
    IMAGE: ghcr.io/strachov/arctic-tilt:cu124-py310-torch26-v0.8.3
    VLLM_WHL_URL: https://github.com/STrachov/OCRlty/releases/download/tilt-vllm-cu124-py310-torch26/vllm-0.8.3-cp310-cp310-linux_x86_64.whl
    VLLM_WHL_SHA256

## 5) RunPod — параметры пода
  **Image:** `ghcr.io/strachov/arctic-tilt:cu124-py310-torch26-v0.8.3`

  **Ports:**
  - 8001,8000

  **Env:**
  ```
  #опционально (все основные переменные окружения и их значения, используемые в коде)
  #VLLM_WHL_URL=https://github.com/STrachov/OCRlty/releases/download/tilt-vllm-cu124-py310-torch26/vllm-0.8.3-cp310-cp310-linux_x86_64.whl
  #VLLM_WHL_NAME=vllm-0.8.3-cp310-cp310-linux_x86_64.whl
  #VLLM_WHL_SHA256=sha256:c0f53b29a7c2b79a86d45fed8770b4164b46dfe5cda5bc4cd375bb86f3335811

  #VLLM_ATTENTION_BACKEND=XFORMERS
  #VLLM_USE_FLASH_ATTENTION=0

  #GIT_URL=https://github.com/STrachov/OCRlty.git
  #GIT_BRANCH=main
  #HF_HOME=/workspace/cache/hf
  #PORT_VLLM=8001
  #PIP_FIND_LINKS=/workspace/wheelhouse
  #PIP_NO_INDEX:=1

  #TILT_MODEL=Snowflake/snowflake-arctic-tilt-v1.3
  #TILT_DTYPE=float16
  #TILT_TP=1 (TP_SIZE)
  #TILT_MAX_MODEL_LEN=16384 (MAX_MODEL_LEN)

  ```

  **Volumes (Network Volume):**
  - `/workspace/cache/hf` — кэши моделей HF (ускоряет рестарты)

  **Command (вариант А — обычный старт):**
  ```
  оставляем пустым
  ```
  **Command (вариант B — отладка):**
  ```
  tail -f /dev/null
  ---

## 6) Dockerfile (prod)
  - Используем devel-вариант базового образа, иначе xformers не соберётся (pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel)
  - Отдельное venv c "тяжёлыми" вещами, чтобы потом переиспользовать
  - vLLM-сборка ставится БЕЗ зависимостей (с --no-deps)
  - Сборка xformers из исходников под именно ЭТУ связку torch/cu124 (используем тот же /opt/venv, тот же python, и nvcc из образа).
  - Копируем готовое /opt/venv из builder, где уже стоят torch, torchvision, vllm и собранный xformers.wheel
  - Ставим собранный xformers (строго из нашего wheel, чтобы не подтянуть чужой)
  - Копируем код приложения и requirements в /opt/app
  - entrypoint — создаёт /workspace/venv, дотягивает runtime-зависимости и стартует uvicorn

## 7) entrypoint.sh
  - Задаем адреса кэшей и базовые пути
  - Всегда используем персистентный venv на /workspace (RunPod volume)
  - Устанавливаем runtime-зависимости проекта в /workspace/venv
  - Делаем пакеты из /opt/venv видимыми (torch, torchvision, vllm, xformers)
  - ВАЖНО: /workspace/venv должен быть первым в PATH, чтобы при желании перекрывать версии лёгких пакетов.
  - Проба окружения: убеждаемся, что ключевые пакеты доступны

## 8) Дальше (опционально)
  - Добавить второй процесс (FastAPI OCR CPU :8000) и объединить в один контейнер через `scripts/entrypoint.sh` (vLLM → wait → API).
  - Ограничить OCR‑параллелизм семафором, держать один `httpx.AsyncClient` к :8001.
  - Вынести общие конфиги в `configs/default.yaml` и переменные в `.env`.
  - Интегрировать e2e‑тесты против :8001 (минимум smoke на `/v1/chat/completions`).
